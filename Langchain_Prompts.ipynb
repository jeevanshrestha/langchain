{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOB+5wzRGKH3SKQ5wLHKXkz",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/jeevanshrestha/langchain/blob/main/Langchain_Prompts.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Langchain Prompts\n"
      ],
      "metadata": {
        "id": "3sNFpkxjO8Jx"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Initial Setup"
      ],
      "metadata": {
        "id": "_dc1LWznPydS"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Install Required Libraries\n"
      ],
      "metadata": {
        "id": "JtMcZrah3tho"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "81A3knUidnaM",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "46dc085d-3c9d-43bb-9a1c-2f0190e9e9e5"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/981.5 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[91m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m522.2/981.5 kB\u001b[0m \u001b[31m15.3 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m981.5/981.5 kB\u001b[0m \u001b[31m15.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.5/2.5 MB\u001b[0m \u001b[31m69.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m65.4/65.4 kB\u001b[0m \u001b[31m4.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.8/1.8 MB\u001b[0m \u001b[31m58.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m31.3/31.3 MB\u001b[0m \u001b[31m59.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m44.4/44.4 kB\u001b[0m \u001b[31m2.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m590.6/590.6 kB\u001b[0m \u001b[31m31.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m167.6/167.6 kB\u001b[0m \u001b[31m10.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.1/3.1 MB\u001b[0m \u001b[31m80.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m195.8/195.8 kB\u001b[0m \u001b[31m12.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m50.9/50.9 kB\u001b[0m \u001b[31m3.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m304.2/304.2 kB\u001b[0m \u001b[31m19.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m114.6/114.6 kB\u001b[0m \u001b[31m7.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Building wheel for langdetect (setup.py) ... \u001b[?25l\u001b[?25hdone\n"
          ]
        }
      ],
      "source": [
        "!pip install openai langchain langchain-community langchain-openai unstructured faiss-cpu -q"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Mount Google Driev"
      ],
      "metadata": {
        "id": "7nHczpwv3qjx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Mount Google Drive to access files\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZdPti1sleYn-",
        "outputId": "d7b26e2f-a559-4dd9-d749-b49841e8fd1c"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "neyTrjSJ3IpB"
      },
      "outputs": [],
      "source": [
        "#set working directory\n",
        "import os\n",
        "os.chdir('/content/drive/MyDrive/GenAI/RAG/')"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### OpenAI Key Setup"
      ],
      "metadata": {
        "id": "ErfzayOVogkP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import userdata\n",
        "api_key = userdata.get('genai_course')"
      ],
      "metadata": {
        "id": "kgYZyIN6eQ4-"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Import Libraries\n"
      ],
      "metadata": {
        "id": "F5XBRsoQ34NR"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "m4IOOMEhF2GK"
      },
      "outputs": [],
      "source": [
        "#Import the libraries\n",
        "from langchain.document_loaders import UnstructuredExcelLoader\n",
        "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
        "from langchain_openai import ChatOpenAI, OpenAIEmbeddings\n",
        "from langchain.vectorstores.faiss import FAISS\n",
        "from IPython.display import display, Markdown\n",
        "import json\n",
        "import numpy as np\n",
        "import pandas as pd"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(os.getcwd())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cYoBiSuwGanC",
        "outputId": "fecf2527-0e4e-4a42-d9c6-2725bc08dea2"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/drive/MyDrive/GenAI/RAG\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Prompts"
      ],
      "metadata": {
        "id": "kFjrZVn_QdQC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_core.prompts import PromptTemplate\n",
        "model = ChatOpenAI(api_key=api_key)\n",
        "\n",
        "# detailed way\n",
        "template = PromptTemplate(\n",
        "    template='Greet this person in 5 languages. The name of the person is {name}',\n",
        "    input_variables=['name']\n",
        ")\n",
        "\n",
        "# fill the values of the placeholders\n",
        "prompt = template.invoke({'name':'Jeevan'})\n",
        "\n",
        "prompt\n",
        "#result = model.invoke(prompt)\n",
        "\n",
        "#print(result.content)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "P4uCmESiQfE4",
        "outputId": "19c4626a-7361-4ea9-8db9-c6b873f178ff"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "StringPromptValue(text='Greet this person in 5 languages. The name of the person is Jeevan')"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Prompt Generator"
      ],
      "metadata": {
        "id": "8jyy4aMqRiu8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# template\n",
        "template = PromptTemplate(\n",
        "    template=\"\"\"\n",
        "Please summarize the research paper titled \"{paper_input}\" with the following specifications:\n",
        "Explanation Style: {style_input}\n",
        "Explanation Length: {length_input}\n",
        "1. Mathematical Details:\n",
        "   - Include relevant mathematical equations if present in the paper.\n",
        "   - Explain the mathematical concepts using simple, intuitive code snippets where applicable.\n",
        "2. Analogies:\n",
        "   - Use relatable analogies to simplify complex ideas.\n",
        "If certain information is not available in the paper, respond with: \"Insufficient information available\" instead of guessing.\n",
        "Ensure the summary is clear, accurate, and aligned with the provided style and length.\n",
        "\"\"\",\n",
        "input_variables=['paper_input', 'style_input','length_input'],\n",
        "validate_template=True\n",
        ")\n",
        "\n",
        "template.save('template.json')"
      ],
      "metadata": {
        "id": "BDxneBJYRZ0x"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Chat Prompt Template"
      ],
      "metadata": {
        "id": "q4pl1EueSMIz"
      }
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "WYW8ycvPXN9F"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "8VWkT49zXiil"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_core.prompts import ChatPromptTemplate\n",
        "chat_template = ChatPromptTemplate([\n",
        "    ('system', 'You are a helpful {domain} expert'),\n",
        "    ('human', 'Explain in simple terms, what is {topic}')\n",
        "])\n",
        "\n",
        "prompt = chat_template.invoke({'domain':'cricket','topic':'Dusra'})\n",
        "\n",
        "prompt"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "O0QsHfG2RhBO",
        "outputId": "198856cf-e4b3-43a5-e750-f8f56beb388e"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "ChatPromptValue(messages=[SystemMessage(content='You are a helpful cricket expert', additional_kwargs={}, response_metadata={}), HumanMessage(content='Explain in simple terms, what is Dusra', additional_kwargs={}, response_metadata={})])"
            ]
          },
          "metadata": {},
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_core.messages import SystemMessage, HumanMessage, AIMessage\n",
        "chat_history = [\n",
        "    SystemMessage(content='You are a helpful AI assistant')\n",
        "]\n",
        "\n",
        "while True:\n",
        "    user_input = input('You: ')\n",
        "    chat_history.append(HumanMessage(content=user_input))\n",
        "    if user_input == 'exit':\n",
        "        break\n",
        "    result = model.invoke(chat_history)\n",
        "    chat_history.append(AIMessage(content=result.content))\n",
        "    print(\"AI: \",result.content)\n",
        "\n",
        "print(chat_history)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HqEdqgOVSTcZ",
        "outputId": "b53da22d-2da7-46a6-f026-55aacbf9506d"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "You: Hello can you help me with coding?\n",
            "AI:  Of course, I'd be happy to help! What do you need assistance with in coding?\n",
            "You: what is gRPC?\n",
            "AI:  gRPC is an open-source remote procedure call (RPC) framework developed by Google. It uses HTTP/2 for transport, Protocol Buffers as the interface description language, and supports multiple programming languages. gRPC allows for efficient communication between distributed systems and is often used in microservices architectures. It enables services to call methods on one another as if they were local functions, making it easier to build scalable and reliable systems.\n",
            "You: exit\n",
            "[SystemMessage(content='You are a helpful AI assistant', additional_kwargs={}, response_metadata={}), HumanMessage(content='Hello can you help me with coding?', additional_kwargs={}, response_metadata={}), AIMessage(content=\"Of course, I'd be happy to help! What do you need assistance with in coding?\", additional_kwargs={}, response_metadata={}), HumanMessage(content='what is gRPC?', additional_kwargs={}, response_metadata={}), AIMessage(content='gRPC is an open-source remote procedure call (RPC) framework developed by Google. It uses HTTP/2 for transport, Protocol Buffers as the interface description language, and supports multiple programming languages. gRPC allows for efficient communication between distributed systems and is often used in microservices architectures. It enables services to call methods on one another as if they were local functions, making it easier to build scalable and reliable systems.', additional_kwargs={}, response_metadata={}), HumanMessage(content='exit', additional_kwargs={}, response_metadata={})]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Streamlit Chat App"
      ],
      "metadata": {
        "id": "Ri9MKGWcS8u8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -q streamlit"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "T_4DB6nvS_Nh",
        "outputId": "091edb41-0089-41b4-e36a-1242e479ca2c"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m44.3/44.3 kB\u001b[0m \u001b[31m1.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m9.9/9.9 MB\u001b[0m \u001b[31m45.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.9/6.9 MB\u001b[0m \u001b[31m61.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m79.1/79.1 kB\u001b[0m \u001b[31m4.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile app.py\n",
        "from langchain_core.prompts import PromptTemplate,load_prompt\n",
        "import streamlit as st\n",
        "from langchain_openai import ChatOpenAI\n",
        "from google.colab import userdata\n",
        "api_key = userdata.get('genai_course')\n",
        "st.header('Reasearch Tool')\n",
        "\n",
        "paper_input = st.selectbox( \"Select Research Paper Name\", [\"Attention Is All You Need\", \"BERT: Pre-training of Deep Bidirectional Transformers\", \"GPT-3: Language Models are Few-Shot Learners\", \"Diffusion Models Beat GANs on Image Synthesis\"] )\n",
        "\n",
        "style_input = st.selectbox( \"Select Explanation Style\", [\"Beginner-Friendly\", \"Technical\", \"Code-Oriented\", \"Mathematical\"] )\n",
        "\n",
        "length_input = st.selectbox( \"Select Explanation Length\", [\"Short (1-2 paragraphs)\", \"Medium (3-5 paragraphs)\", \"Long (detailed explanation)\"] )\n",
        "\n",
        "template = load_prompt('template.json')\n",
        "\n",
        "api_key = userdata.get('genai_course')\n",
        "model = ChatOpenAI(api_key=api_key)\n",
        "\n",
        "if st.button('Summarize'):\n",
        "    chain = template | model\n",
        "    result = chain.invoke({\n",
        "        'paper_input':paper_input,\n",
        "        'style_input':style_input,\n",
        "        'length_input':length_input\n",
        "    })\n",
        "    st.write(result.content)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-LkrObpOTBF8",
        "outputId": "fb04ff49-862f-4c62-bee8-8230b70b581c"
      },
      "execution_count": 50,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Overwriting app.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!streamlit run app.py &>/content/logs.txt & curl ipv4.icanhazip.com\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vTsdxw3-TEFi",
        "outputId": "03e7922d-adcf-41b7-c9a3-05c14570799d"
      },
      "execution_count": 51,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "34.81.193.93\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!npx localtunnel --port 8501\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "b25kxZKdTGCh",
        "outputId": "08761296-a92f-400a-94ee-318e180b6e1b"
      },
      "execution_count": 52,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1G\u001b[0K⠙\u001b[1G\u001b[0K⠹\u001b[1G\u001b[0K⠸\u001b[1G\u001b[0K⠼\u001b[1G\u001b[0K⠴\u001b[1G\u001b[0K⠦\u001b[1G\u001b[0K⠧\u001b[1G\u001b[0K⠇\u001b[1G\u001b[0K⠏\u001b[1G\u001b[0K⠋\u001b[1G\u001b[0K⠙\u001b[1G\u001b[0K⠹\u001b[1G\u001b[0Kyour url is: https://vast-turkeys-lay.loca.lt\n",
            "^C\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Message Placeholder\n"
      ],
      "metadata": {
        "id": "0gmKS8cHXk-M"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_core.messages import SystemMessage, HumanMessage, AIMessage\n",
        "\n",
        "# Read from file or database\n",
        "chat_history = [\n",
        "    HumanMessage(content=\"I want to request a refund for my order #12345.\"),\n",
        "    AIMessage(content=\"Your refund request for order #12345 has been initiated. It will be processed in 3-5 business days.\")\n",
        "    ]\n",
        "\n",
        "from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder\n",
        "# chat template\n",
        "chat_template = ChatPromptTemplate([\n",
        "    ('system','You are a helpful customer support agent'),\n",
        "    MessagesPlaceholder(variable_name='chat_history'),\n",
        "    ('human','{query}')\n",
        "])\n",
        "\n",
        "# create prompt\n",
        "prompt = chat_template.invoke({'chat_history':chat_history, 'query':'Where is my refund'})\n",
        "\n",
        "print(prompt)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_Kuf6CRIVKRu",
        "outputId": "3916d1b1-4739-4b4b-b3a1-c9b8add8c048"
      },
      "execution_count": 54,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "messages=[SystemMessage(content='You are a helpful customer support agent', additional_kwargs={}, response_metadata={}), HumanMessage(content='I want to request a refund for my order #12345.', additional_kwargs={}, response_metadata={}), AIMessage(content='Your refund request for order #12345 has been initiated. It will be processed in 3-5 business days.', additional_kwargs={}, response_metadata={}), HumanMessage(content='Where is my refund', additional_kwargs={}, response_metadata={})]\n"
          ]
        }
      ]
    }
  ]
}