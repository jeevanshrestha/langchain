{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "_dc1LWznPydS",
        "JtMcZrah3tho"
      ],
      "authorship_tag": "ABX9TyOde79OnS7+nUd4us3fALS8",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/jeevanshrestha/langchain/blob/main/Langchain_Runnables.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Langchain Runnables"
      ],
      "metadata": {
        "id": "3sNFpkxjO8Jx"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Initial Setup"
      ],
      "metadata": {
        "id": "_dc1LWznPydS"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Install Required Libraries\n"
      ],
      "metadata": {
        "id": "JtMcZrah3tho"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "81A3knUidnaM",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d4c97cd9-8799-4be6-dba1-c683de469ec6"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[?25l     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m0.0/981.5 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[91mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[90mâ•º\u001b[0m \u001b[32m962.6/981.5 kB\u001b[0m \u001b[31m34.0 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K     \u001b[91mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[91mâ•¸\u001b[0m \u001b[32m972.8/981.5 kB\u001b[0m \u001b[31m21.7 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m981.5/981.5 kB\u001b[0m \u001b[31m9.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m2.5/2.5 MB\u001b[0m \u001b[31m47.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m69.0/69.0 kB\u001b[0m \u001b[31m4.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m1.8/1.8 MB\u001b[0m \u001b[31m47.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m31.3/31.3 MB\u001b[0m \u001b[31m23.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m44.4/44.4 kB\u001b[0m \u001b[31m2.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m590.6/590.6 kB\u001b[0m \u001b[31m25.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m167.6/167.6 kB\u001b[0m \u001b[31m9.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m3.1/3.1 MB\u001b[0m \u001b[31m51.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m195.8/195.8 kB\u001b[0m \u001b[31m10.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m50.9/50.9 kB\u001b[0m \u001b[31m2.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m304.2/304.2 kB\u001b[0m \u001b[31m15.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m114.6/114.6 kB\u001b[0m \u001b[31m7.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Building wheel for langdetect (setup.py) ... \u001b[?25l\u001b[?25hdone\n"
          ]
        }
      ],
      "source": [
        "!pip install openai langchain langchain-community langchain_huggingface langchain-openai unstructured faiss-cpu -q"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Mount Google Driev"
      ],
      "metadata": {
        "id": "7nHczpwv3qjx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Mount Google Drive to access files\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZdPti1sleYn-",
        "outputId": "fbbe0901-6b1d-4962-c2d0-d5202b7fd7f4"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "neyTrjSJ3IpB"
      },
      "outputs": [],
      "source": [
        "#set working directory\n",
        "import os\n",
        "os.chdir('/content/drive/MyDrive/GenAI/RAG/')"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### OpenAI Key Setup"
      ],
      "metadata": {
        "id": "ErfzayOVogkP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import userdata\n",
        "api_key = userdata.get('genai_course')"
      ],
      "metadata": {
        "id": "kgYZyIN6eQ4-"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Import Libraries\n"
      ],
      "metadata": {
        "id": "F5XBRsoQ34NR"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "m4IOOMEhF2GK"
      },
      "outputs": [],
      "source": [
        "#Import the libraries\n",
        "from langchain_openai import ChatOpenAI\n",
        "from IPython.display import display, Markdown\n",
        "import json\n",
        "import numpy as np\n",
        "import pandas as pd"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(os.getcwd())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cYoBiSuwGanC",
        "outputId": "c858ca10-fddc-4f2c-e7da-5f389f65ddfe"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/drive/MyDrive/GenAI/RAG\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Runnable Sequence"
      ],
      "metadata": {
        "id": "_NYThhCgZIra"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_openai import ChatOpenAI\n",
        "from langchain_core.prompts import PromptTemplate\n",
        "from langchain_core.output_parsers import StrOutputParser\n",
        "from dotenv import load_dotenv\n",
        "from langchain.schema.runnable import RunnableSequence\n",
        "\n",
        "\n",
        "prompt1 = PromptTemplate(\n",
        "    template='Write a joke about {topic}',\n",
        "    input_variables=['topic']\n",
        ")\n",
        "\n",
        "model = ChatOpenAI(api_key=api_key)\n",
        "\n",
        "parser = StrOutputParser()\n",
        "\n",
        "prompt2 = PromptTemplate(\n",
        "    template='Explain the following joke - {text}',\n",
        "    input_variables=['text']\n",
        ")\n",
        "\n",
        "chain = RunnableSequence(prompt1, model, parser, prompt2, model, parser)\n",
        "result = chain.invoke({'topic':'AI'})"
      ],
      "metadata": {
        "id": "QL0rQc09ZNtf"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "result"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 70
        },
        "id": "8lK5fxIHZtn5",
        "outputId": "c12891fd-c09c-46a3-98bd-f3d9d2511b7a"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'This joke is playing on the idea that going to school can often make someone more knowledgeable or intelligent, and in this case, the AI (artificial intelligence) is going to school to upgrade its system and become a \"smarty\" pants, a play on the phrase \"smarty pants\" which means someone who is very smart or intelligent. The joke highlights the idea that even though AI is already advanced, it can always benefit from further education and upgrades to improve its capabilities.'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install grandalf -q\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "muzniNR7Z4Sv",
        "outputId": "46751114-fdff-46b2-d554-3a25665d6348"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[?25l   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m0.0/41.8 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m41.8/41.8 kB\u001b[0m \u001b[31m1.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "chain.get_graph().print_ascii()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZJV_Cu9ZZu3o",
        "outputId": "5d8c0279-654d-4101-e945-f1e10a4ab13d"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "     +-------------+       \n",
            "     | PromptInput |       \n",
            "     +-------------+       \n",
            "            *              \n",
            "            *              \n",
            "            *              \n",
            "    +----------------+     \n",
            "    | PromptTemplate |     \n",
            "    +----------------+     \n",
            "            *              \n",
            "            *              \n",
            "            *              \n",
            "      +------------+       \n",
            "      | ChatOpenAI |       \n",
            "      +------------+       \n",
            "            *              \n",
            "            *              \n",
            "            *              \n",
            "   +-----------------+     \n",
            "   | StrOutputParser |     \n",
            "   +-----------------+     \n",
            "            *              \n",
            "            *              \n",
            "            *              \n",
            "+-----------------------+  \n",
            "| StrOutputParserOutput |  \n",
            "+-----------------------+  \n",
            "            *              \n",
            "            *              \n",
            "            *              \n",
            "    +----------------+     \n",
            "    | PromptTemplate |     \n",
            "    +----------------+     \n",
            "            *              \n",
            "            *              \n",
            "            *              \n",
            "      +------------+       \n",
            "      | ChatOpenAI |       \n",
            "      +------------+       \n",
            "            *              \n",
            "            *              \n",
            "            *              \n",
            "   +-----------------+     \n",
            "   | StrOutputParser |     \n",
            "   +-----------------+     \n",
            "            *              \n",
            "            *              \n",
            "            *              \n",
            "+-----------------------+  \n",
            "| StrOutputParserOutput |  \n",
            "+-----------------------+  \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "vqJ3aMWsZywn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Runnable Parallel"
      ],
      "metadata": {
        "id": "UzBajc4ZaJ97"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_openai import ChatOpenAI\n",
        "from langchain_core.prompts import PromptTemplate\n",
        "from langchain_core.output_parsers import StrOutputParser\n",
        "from dotenv import load_dotenv\n",
        "from langchain.schema.runnable import RunnableSequence, RunnableParallel\n",
        "\n",
        "\n",
        "prompt1 = PromptTemplate(\n",
        "    template='Generate a tweet about {topic}',\n",
        "    input_variables=['topic']\n",
        ")\n",
        "\n",
        "prompt2 = PromptTemplate(\n",
        "    template='Generate a Linkedin post about {topic}',\n",
        "    input_variables=['topic']\n",
        ")\n",
        "\n",
        "model = ChatOpenAI(api_key=api_key)\n",
        "\n",
        "parser = StrOutputParser()\n",
        "\n",
        "parallel_chain = RunnableParallel({\n",
        "    'tweet': RunnableSequence(prompt1, model, parser),\n",
        "    'linkedin': RunnableSequence(prompt2, model, parser)\n",
        "})\n",
        "\n",
        "result = parallel_chain.invoke({'topic':'AI'})\n",
        "\n",
        "print(result['tweet'])\n",
        "print(result['linkedin'])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wM6m7VHQaN8h",
        "outputId": "bcdc2d30-d140-464c-9cc4-5795dd56d5d8"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\"Just witnessed the amazing potential of #AI in action - the speed and accuracy in decision-making is truly mind-blowing. Excited to see where this technology takes us next! ğŸ¤–ğŸ’¡ #ArtificialIntelligence\"\n",
            "Exciting news in the world of AI! Advances in artificial intelligence continue to revolutionize industries and drive innovation. From machine learning algorithms to natural language processing, AI is reshaping the way businesses operate. As we embrace this technology, the possibilities are endless. Stay ahead of the curve and learn how AI can transform your industry. #AI #ArtificialIntelligence #Innovation #Technology #MachineLearning.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "result"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nNxNszhHaRWg",
        "outputId": "faf898c3-3b35-4e29-813b-f9cba13b4da4"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'tweet': '\"Just witnessed the amazing potential of #AI in action - the speed and accuracy in decision-making is truly mind-blowing. Excited to see where this technology takes us next! ğŸ¤–ğŸ’¡ #ArtificialIntelligence\"',\n",
              " 'linkedin': 'Exciting news in the world of AI! Advances in artificial intelligence continue to revolutionize industries and drive innovation. From machine learning algorithms to natural language processing, AI is reshaping the way businesses operate. As we embrace this technology, the possibilities are endless. Stay ahead of the curve and learn how AI can transform your industry. #AI #ArtificialIntelligence #Innovation #Technology #MachineLearning.'}"
            ]
          },
          "metadata": {},
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "parallel_chain.get_graph().print_ascii()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0auzJRNfaY68",
        "outputId": "e69a48f8-21b1-4364-9bed-624554e6502e"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "          +-------------------------------+          \n",
            "          | Parallel<tweet,linkedin>Input |          \n",
            "          +-------------------------------+          \n",
            "                 **               **                 \n",
            "              ***                   ***              \n",
            "            **                         **            \n",
            "+----------------+                +----------------+ \n",
            "| PromptTemplate |                | PromptTemplate | \n",
            "+----------------+                +----------------+ \n",
            "          *                               *          \n",
            "          *                               *          \n",
            "          *                               *          \n",
            "  +------------+                    +------------+   \n",
            "  | ChatOpenAI |                    | ChatOpenAI |   \n",
            "  +------------+                    +------------+   \n",
            "          *                               *          \n",
            "          *                               *          \n",
            "          *                               *          \n",
            "+-----------------+              +-----------------+ \n",
            "| StrOutputParser |              | StrOutputParser | \n",
            "+-----------------+              +-----------------+ \n",
            "                 **               **                 \n",
            "                   ***         ***                   \n",
            "                      **     **                      \n",
            "         +--------------------------------+          \n",
            "         | Parallel<tweet,linkedin>Output |          \n",
            "         +--------------------------------+          \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Sl53qkNYabYy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Runnable Branch"
      ],
      "metadata": {
        "id": "IvRp-KB0bC6E"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_openai import ChatOpenAI\n",
        "from langchain_core.prompts import PromptTemplate\n",
        "from langchain_core.output_parsers import StrOutputParser\n",
        "from dotenv import load_dotenv\n",
        "from langchain.schema.runnable import RunnableSequence, RunnableParallel, RunnablePassthrough, RunnableBranch, RunnableLambda\n",
        "\n",
        "load_dotenv()\n",
        "\n",
        "prompt1 = PromptTemplate(\n",
        "    template='Write a detailed report on {topic}',\n",
        "    input_variables=['topic']\n",
        ")\n",
        "\n",
        "prompt2 = PromptTemplate(\n",
        "    template='Summarize the following text \\n {text}',\n",
        "    input_variables=['text']\n",
        ")\n",
        "\n",
        "model = ChatOpenAI(api_key=api_key)\n",
        "\n",
        "parser = StrOutputParser()\n",
        "\n",
        "report_gen_chain = prompt1 | model | parser\n",
        "\n",
        "branch_chain = RunnableBranch(\n",
        "    (lambda x: len(x.split())>300, prompt2 | model | parser),\n",
        "    RunnablePassthrough()\n",
        ")\n",
        "\n",
        "final_chain = RunnableSequence(report_gen_chain, branch_chain)\n",
        "\n",
        "result =final_chain.invoke({'topic':'Russia vs Ukraine'})\n"
      ],
      "metadata": {
        "id": "ZjtI8FlvbEO3"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "result"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 105
        },
        "id": "SHi632I2bMIE",
        "outputId": "29521949-89d3-4145-e162-92f559f28342"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "\"The conflict between Russia and Ukraine, which began in 2014 with the annexation of Crimea, has far-reaching implications for both countries and the international community. The historical background of the conflict dates back to Ukraine's independence from the Soviet Union in 1991, with tensions escalating when Ukraine sought closer ties with the EU and NATO. Since 2014, the conflict has resulted in thousands of deaths, economic damage, and ongoing hostilities, with sporadic fighting in eastern Ukraine. Potential solutions include a negotiated settlement respecting Ukraine's sovereignty or Russia withdrawing support for separatist rebels. International efforts to mediate the conflict continue, but progress has been limited. A peaceful resolution is urgently needed to address the human and economic costs of the conflict and respect the rights and interests of both Russia and Ukraine.\""
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 19
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "final_chain.get_graph().print_ascii()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5RjWGNAXbM76",
        "outputId": "17e1694b-657c-469c-e52d-7b8a80a8761e"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  +-------------+    \n",
            "  | PromptInput |    \n",
            "  +-------------+    \n",
            "          *          \n",
            "          *          \n",
            "          *          \n",
            "+----------------+   \n",
            "| PromptTemplate |   \n",
            "+----------------+   \n",
            "          *          \n",
            "          *          \n",
            "          *          \n",
            "  +------------+     \n",
            "  | ChatOpenAI |     \n",
            "  +------------+     \n",
            "          *          \n",
            "          *          \n",
            "          *          \n",
            "+-----------------+  \n",
            "| StrOutputParser |  \n",
            "+-----------------+  \n",
            "          *          \n",
            "          *          \n",
            "          *          \n",
            "    +--------+       \n",
            "    | Branch |       \n",
            "    +--------+       \n",
            "          *          \n",
            "          *          \n",
            "          *          \n",
            "  +--------------+   \n",
            "  | BranchOutput |   \n",
            "  +--------------+   \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "o6ewJud9bRzl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Runnable Lambda"
      ],
      "metadata": {
        "id": "zHwGK54ebXCp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_openai import ChatOpenAI\n",
        "from langchain_core.prompts import PromptTemplate\n",
        "from langchain_core.output_parsers import StrOutputParser\n",
        "from dotenv import load_dotenv\n",
        "from langchain.schema.runnable import RunnableSequence, RunnableLambda, RunnablePassthrough, RunnableParallel\n",
        "\n",
        "load_dotenv()\n",
        "\n",
        "#Custom Python Function\n",
        "\n",
        "def word_count(text):\n",
        "    return len(text.split())\n",
        "\n",
        "prompt = PromptTemplate(\n",
        "    template='Write a joke about {topic}',\n",
        "    input_variables=['topic']\n",
        ")\n",
        "\n",
        "model = ChatOpenAI(api_key=api_key)\n",
        "\n",
        "parser = StrOutputParser()\n",
        "\n",
        "joke_gen_chain = RunnableSequence(prompt, model, parser)\n",
        "\n",
        "parallel_chain = RunnableParallel({\n",
        "    'joke': RunnablePassthrough(),\n",
        "    'word_count': RunnableLambda(word_count) # Wrap in RunnableLambda to make the custom function into runnbale\n",
        "})\n",
        "\n",
        "final_chain = RunnableSequence(joke_gen_chain, parallel_chain)\n",
        "\n",
        "result = final_chain.invoke({'topic':'AI'})\n",
        "\n",
        "final_result = \"\"\"{} \\n word count - {}\"\"\".format(result['joke'], result['word_count'])\n",
        "\n",
        "print(final_result)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4bDcFMuqbYkt",
        "outputId": "59fa4856-2670-4d1c-ed24-53d2caf15082"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Why did the AI break up with its calculator girlfriend?\n",
            "\n",
            "Because she couldn't handle its complex algorithms! \n",
            " word count - 17\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "final_chain.get_graph().print_ascii()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hkfHJeQDbp5C",
        "outputId": "2c234467-911c-4f04-b0f1-628c5bb13f66"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "              +-------------+                \n",
            "              | PromptInput |                \n",
            "              +-------------+                \n",
            "                      *                      \n",
            "                      *                      \n",
            "                      *                      \n",
            "             +----------------+              \n",
            "             | PromptTemplate |              \n",
            "             +----------------+              \n",
            "                      *                      \n",
            "                      *                      \n",
            "                      *                      \n",
            "               +------------+                \n",
            "               | ChatOpenAI |                \n",
            "               +------------+                \n",
            "                      *                      \n",
            "                      *                      \n",
            "                      *                      \n",
            "            +-----------------+              \n",
            "            | StrOutputParser |              \n",
            "            +-----------------+              \n",
            "                      *                      \n",
            "                      *                      \n",
            "                      *                      \n",
            "     +--------------------------------+      \n",
            "     | Parallel<joke,word_count>Input |      \n",
            "     +--------------------------------+      \n",
            "              **            ***              \n",
            "            **                 **            \n",
            "          **                     **          \n",
            "+-------------+              +------------+  \n",
            "| Passthrough |              | word_count |  \n",
            "+-------------+              +------------+  \n",
            "              **            ***              \n",
            "                **        **                 \n",
            "                  **    **                   \n",
            "    +---------------------------------+      \n",
            "    | Parallel<joke,word_count>Output |      \n",
            "    +---------------------------------+      \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "84PlMTgxbxe4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Runnable PassThrough"
      ],
      "metadata": {
        "id": "2g64hmrPb0g2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_openai import ChatOpenAI\n",
        "from langchain_core.prompts import PromptTemplate\n",
        "from langchain_core.output_parsers import StrOutputParser\n",
        "from dotenv import load_dotenv\n",
        "from langchain.schema.runnable import RunnableSequence, RunnableParallel, RunnablePassthrough\n",
        "\n",
        "load_dotenv()\n",
        "\n",
        "prompt1 = PromptTemplate(\n",
        "    template='Write a joke about {topic}',\n",
        "    input_variables=['topic']\n",
        ")\n",
        "\n",
        "model = ChatOpenAI(api_key=api_key)\n",
        "\n",
        "parser = StrOutputParser()\n",
        "\n",
        "prompt2 = PromptTemplate(\n",
        "    template='Explain the following joke - {text}',\n",
        "    input_variables=['text']\n",
        ")\n",
        "\n",
        "joke_gen_chain = RunnableSequence(prompt1, model, parser)\n",
        "\n",
        "parallel_chain = RunnableParallel({\n",
        "    'joke': RunnablePassthrough(),\n",
        "    'explanation': RunnableSequence(prompt2, model, parser)\n",
        "})\n",
        "\n",
        "final_chain = RunnableSequence(joke_gen_chain, parallel_chain)\n",
        "\n",
        "print(final_chain.invoke({'topic':'cricket'}))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TbIS80Vpb27H",
        "outputId": "99e41cbe-5026-43c0-e315-41ad1ef701a1"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'joke': 'Why did the cricket team go to the dermatologist?\\n\\nBecause they had too many runs!', 'explanation': 'This joke is a play on words. In cricket, a \"run\" is a point scored by the batsmen running between the wickets. However, in dermatology, a \"run\" refers to a rash or outbreak on the skin. So, when the cricket team had \"too many runs,\" it can be interpreted as both scoring too many points in a cricket match and also having too many rashes on their skin. This is why they needed to go to the dermatologist.'}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "final_chain.get_graph().print_ascii()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nGxYhFxab6o8",
        "outputId": "da68dd1d-b115-426b-a2f1-866f1b3d5eb2"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                  +-------------+                  \n",
            "                  | PromptInput |                  \n",
            "                  +-------------+                  \n",
            "                          *                        \n",
            "                          *                        \n",
            "                          *                        \n",
            "                +----------------+                 \n",
            "                | PromptTemplate |                 \n",
            "                +----------------+                 \n",
            "                          *                        \n",
            "                          *                        \n",
            "                          *                        \n",
            "                  +------------+                   \n",
            "                  | ChatOpenAI |                   \n",
            "                  +------------+                   \n",
            "                          *                        \n",
            "                          *                        \n",
            "                          *                        \n",
            "                +-----------------+                \n",
            "                | StrOutputParser |                \n",
            "                +-----------------+                \n",
            "                          *                        \n",
            "                          *                        \n",
            "                          *                        \n",
            "        +---------------------------------+        \n",
            "        | Parallel<joke,explanation>Input |        \n",
            "        +---------------------------------+        \n",
            "                 **              ***               \n",
            "              ***                   **             \n",
            "            **                        ***          \n",
            "+----------------+                       **        \n",
            "| PromptTemplate |                        *        \n",
            "+----------------+                        *        \n",
            "          *                               *        \n",
            "          *                               *        \n",
            "          *                               *        \n",
            "  +------------+                          *        \n",
            "  | ChatOpenAI |                          *        \n",
            "  +------------+                          *        \n",
            "          *                               *        \n",
            "          *                               *        \n",
            "          *                               *        \n",
            "+-----------------+               +-------------+  \n",
            "| StrOutputParser |               | Passthrough |  \n",
            "+-----------------+               +-------------+  \n",
            "                 **              **                \n",
            "                   ***        ***                  \n",
            "                      **    **                     \n",
            "        +----------------------------------+       \n",
            "        | Parallel<joke,explanation>Output |       \n",
            "        +----------------------------------+       \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "JIu-dYCKcBGD"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}