{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyO29jYC5OAk603EXb0Bazoj",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/jeevanshrestha/langchain/blob/main/Langchain_Chains.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Langchain Chains"
      ],
      "metadata": {
        "id": "3sNFpkxjO8Jx"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Initial Setup"
      ],
      "metadata": {
        "id": "_dc1LWznPydS"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Install Required Libraries\n"
      ],
      "metadata": {
        "id": "JtMcZrah3tho"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "81A3knUidnaM",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ae327513-fb4c-4228-f9d8-d8d883ed7c6e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/41.8 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m41.8/41.8 kB\u001b[0m \u001b[31m2.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ],
      "source": [
        "!pip install openai langchain langchain-community langchain_huggingface langchain-openai unstructured faiss-cpu -q\n",
        "!pip install grandalf -q"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Mount Google Driev"
      ],
      "metadata": {
        "id": "7nHczpwv3qjx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Mount Google Drive to access files\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZdPti1sleYn-",
        "outputId": "17a9c7ca-0973-42c5-e47e-518b4b3583e2"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "neyTrjSJ3IpB"
      },
      "outputs": [],
      "source": [
        "#set working directory\n",
        "import os\n",
        "os.chdir('/content/drive/MyDrive/GenAI/RAG/')"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### OpenAI Key Setup"
      ],
      "metadata": {
        "id": "ErfzayOVogkP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import userdata\n",
        "api_key = userdata.get('genai_course')"
      ],
      "metadata": {
        "id": "kgYZyIN6eQ4-"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Import Libraries\n"
      ],
      "metadata": {
        "id": "F5XBRsoQ34NR"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "m4IOOMEhF2GK"
      },
      "outputs": [],
      "source": [
        "#Import the libraries\n",
        "from langchain_openai import ChatOpenAI, OpenAIEmbeddings\n",
        "from IPython.display import display, Markdown\n",
        "import json\n",
        "import numpy as np\n",
        "import pandas as pd"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(os.getcwd())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cYoBiSuwGanC",
        "outputId": "22ab1279-46a7-4649-8111-f9dfda6d251a"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/drive/MyDrive/GenAI/RAG\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Simple Chains"
      ],
      "metadata": {
        "id": "zHqHvmH2ibYk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "from langchain_core.prompts import PromptTemplate\n",
        "from langchain_core.output_parsers import StrOutputParser\n",
        "\n",
        "prompt = PromptTemplate(\n",
        "    template='Generate 5 interesting facts about {topic}',\n",
        "    input_variables=['topic']\n",
        ")\n",
        "\n",
        "model = ChatOpenAI(api_key=api_key)\n",
        "\n",
        "parser = StrOutputParser()\n",
        "\n",
        "chain = prompt | model | parser\n",
        "\n",
        "result = chain.invoke({'topic':'cricket'})"
      ],
      "metadata": {
        "id": "WFoxz-1qictG"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "print(result)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Gj7vZXQcjI24",
        "outputId": "72b49a84-b584-4f00-86cb-436ee281103d"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1. Cricket is the second most popular sport in the world, with over 2.5 billion fans globally, second only to soccer.\n",
            "2. The longest cricket match in history lasted for a grueling 14 days, played between England and South Africa in 1939.\n",
            "3. The highest individual score in a Test match is held by Brian Lara, who scored 400 runs in a single innings against England in 2004.\n",
            "4. The Ashes is one of the oldest and most prestigious cricket series, played between England and Australia since 1882.\n",
            "5. The sport of cricket originated in England in the 16th century and has since spread to over 100 countries, becoming particularly popular in countries like India, Australia, and Pakistan.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "chain.get_graph().print_ascii()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JQ0dkkt6jWfZ",
        "outputId": "5ed64b62-68d9-44e4-eb63-c4379838a94a"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "     +-------------+       \n",
            "     | PromptInput |       \n",
            "     +-------------+       \n",
            "            *              \n",
            "            *              \n",
            "            *              \n",
            "    +----------------+     \n",
            "    | PromptTemplate |     \n",
            "    +----------------+     \n",
            "            *              \n",
            "            *              \n",
            "            *              \n",
            "      +------------+       \n",
            "      | ChatOpenAI |       \n",
            "      +------------+       \n",
            "            *              \n",
            "            *              \n",
            "            *              \n",
            "   +-----------------+     \n",
            "   | StrOutputParser |     \n",
            "   +-----------------+     \n",
            "            *              \n",
            "            *              \n",
            "            *              \n",
            "+-----------------------+  \n",
            "| StrOutputParserOutput |  \n",
            "+-----------------------+  \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "uTiYzmA7jYtT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Sequential Chain"
      ],
      "metadata": {
        "id": "xDqHBfRkjlK9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "from langchain_core.prompts import PromptTemplate\n",
        "from langchain_core.output_parsers import StrOutputParser\n",
        "\n",
        "\n",
        "prompt1 = PromptTemplate(\n",
        "    template='Generate a detailed report on {topic}',\n",
        "    input_variables=['topic']\n",
        ")\n",
        "\n",
        "prompt2 = PromptTemplate(\n",
        "    template='Generate a 5 pointer summary from the following text \\n {text}',\n",
        "    input_variables=['text']\n",
        ")\n",
        "\n",
        "model = ChatOpenAI(api_key = api_key)\n",
        "\n",
        "parser = StrOutputParser()\n",
        "\n",
        "chain = prompt1 | model | parser | prompt2 | model | parser\n",
        "\n",
        "result = chain.invoke({'topic': 'Unemployment in India'})\n"
      ],
      "metadata": {
        "id": "1XTiuKbejmZQ"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "print(result)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NwLK140vjsQv",
        "outputId": "fd582a3a-253a-4fa8-84d9-341487b47e44"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1. The unemployment rate in India was 7.7% in April 2021, with youth unemployment at 22.5%.\n",
            "2. Lack of job creation in the formal sector, skills gap, and Covid-19 pandemic are major causes of unemployment in India.\n",
            "3. Potential solutions include encouraging entrepreneurship, investing in education, promoting job creation in key sectors, labor market reforms, and enhancing social protection programs.\n",
            "4. India needs a multi-faceted approach involving government, private sector, and civil society to address unemployment effectively.\n",
            "5. Collaboration and prioritization of job creation and skills development are crucial for reducing unemployment and providing better opportunities for the workforce in India.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "chain.get_graph().print_ascii()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "07lbssaojtXG",
        "outputId": "bb1e4ddd-6631-439c-ead3-7774e4711b23"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "     +-------------+       \n",
            "     | PromptInput |       \n",
            "     +-------------+       \n",
            "            *              \n",
            "            *              \n",
            "            *              \n",
            "    +----------------+     \n",
            "    | PromptTemplate |     \n",
            "    +----------------+     \n",
            "            *              \n",
            "            *              \n",
            "            *              \n",
            "      +------------+       \n",
            "      | ChatOpenAI |       \n",
            "      +------------+       \n",
            "            *              \n",
            "            *              \n",
            "            *              \n",
            "   +-----------------+     \n",
            "   | StrOutputParser |     \n",
            "   +-----------------+     \n",
            "            *              \n",
            "            *              \n",
            "            *              \n",
            "+-----------------------+  \n",
            "| StrOutputParserOutput |  \n",
            "+-----------------------+  \n",
            "            *              \n",
            "            *              \n",
            "            *              \n",
            "    +----------------+     \n",
            "    | PromptTemplate |     \n",
            "    +----------------+     \n",
            "            *              \n",
            "            *              \n",
            "            *              \n",
            "      +------------+       \n",
            "      | ChatOpenAI |       \n",
            "      +------------+       \n",
            "            *              \n",
            "            *              \n",
            "            *              \n",
            "   +-----------------+     \n",
            "   | StrOutputParser |     \n",
            "   +-----------------+     \n",
            "            *              \n",
            "            *              \n",
            "            *              \n",
            "+-----------------------+  \n",
            "| StrOutputParserOutput |  \n",
            "+-----------------------+  \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "0w8vEs90jw1a"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Parallel Chains"
      ],
      "metadata": {
        "id": "RIweWS5sj54B"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "from langchain_core.prompts import PromptTemplate\n",
        "from langchain_core.output_parsers import StrOutputParser\n",
        "from langchain.schema.runnable import RunnableParallel\n",
        "from langchain_huggingface import ChatHuggingFace, HuggingFaceEndpoint\n",
        "\n",
        "model1 = ChatOpenAI(api_key=api_key)\n",
        "\n",
        "# Define the model\n",
        "llm = HuggingFaceEndpoint(\n",
        "    repo_id=\"google/gemma-2-2b-it\",\n",
        "    task=\"text-generation\"\n",
        ")\n",
        "\n",
        "model2 = ChatHuggingFace(llm=llm)\n",
        "\n",
        "prompt1 = PromptTemplate(\n",
        "    template='Generate short and simple notes from the following text \\n {text}',\n",
        "    input_variables=['text']\n",
        ")\n",
        "\n",
        "prompt2 = PromptTemplate(\n",
        "    template='Generate 5 short question answers from the following text \\n {text}',\n",
        "    input_variables=['text']\n",
        ")\n",
        "\n",
        "prompt3 = PromptTemplate(\n",
        "    template='Merge the provided notes and quiz into a single document \\n notes -> {notes} and quiz -> {quiz}',\n",
        "    input_variables=['notes', 'quiz']\n",
        ")\n",
        "\n",
        "parser = StrOutputParser()\n",
        "\n",
        "parallel_chain = RunnableParallel({\n",
        "    'notes': prompt1 | model1 | parser,\n",
        "    'quiz': prompt2 | model2 | parser\n",
        "})\n",
        "\n",
        "merge_chain = prompt3 | model1 | parser\n",
        "\n",
        "chain = parallel_chain | merge_chain\n",
        "\n",
        "text = \"\"\"\n",
        "Support vector machines (SVMs) are a set of supervised learning methods used for classification, regression and outliers detection.\n",
        "\n",
        "The advantages of support vector machines are:\n",
        "\n",
        "Effective in high dimensional spaces.\n",
        "\n",
        "Still effective in cases where number of dimensions is greater than the number of samples.\n",
        "\n",
        "Uses a subset of training points in the decision function (called support vectors), so it is also memory efficient.\n",
        "\n",
        "Versatile: different Kernel functions can be specified for the decision function. Common kernels are provided, but it is also possible to specify custom kernels.\n",
        "\n",
        "The disadvantages of support vector machines include:\n",
        "\n",
        "If the number of features is much greater than the number of samples, avoid over-fitting in choosing Kernel functions and regularization term is crucial.\n",
        "\n",
        "SVMs do not directly provide probability estimates, these are calculated using an expensive five-fold cross-validation (see Scores and probabilities, below).\n",
        "\n",
        "The support vector machines in scikit-learn support both dense (numpy.ndarray and convertible to that by numpy.asarray) and sparse (any scipy.sparse) sample vectors as input. However, to use an SVM to make predictions for sparse data, it must have been fit on such data. For optimal performance, use C-ordered numpy.ndarray (dense) or scipy.sparse.csr_matrix (sparse) with dtype=float64.\n",
        "\"\"\"\n",
        "\n",
        "result = chain.invoke({'text':text})\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "GV1GLYqDj7Nt"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(result)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "O6s-oOsbj85J",
        "outputId": "d88ed7da-b32c-4ddb-fd5f-f9594d2f833b"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Notes:\n",
            "\n",
            "- Support vector machines (SVMs) are used for classification, regression, and outlier detection.\n",
            "- Advantages of SVMs include effectiveness in high dimensional spaces, memory efficiency, and versatility with different Kernel functions.\n",
            "- Disadvantages of SVMs include potential overfitting with too many features and the lack of direct probability estimates.\n",
            "- It is recommended to use dense or sparse sample vectors as input for optimal SVM performance.\n",
            "\n",
            "Quiz:\n",
            "\n",
            "1. Q: What are the key tasks support vector machines (SVMs) excel at?\n",
            "A: SVMs are adept at classification, regression, and outlier detection.\n",
            "\n",
            "2. Q: How do SVMs address the challenge of high-dimensional data?\n",
            "A: SVMs are effective in high-dimensional spaces because they leverage a subset of training points for the decision function (support vectors).\n",
            "\n",
            "3. Q: What distinguishes SVM's memory efficiency?\n",
            "A: Support vector machines are memory efficient because they use a subset of training points in their decision function.\n",
            "\n",
            "4. Q: Why are kernel functions crucial in SVM?\n",
            "A: Kernel functions govern how data is mapped into higher-dimensional space, and they are particularly important when the number of features is large compared to the number of samples.\n",
            "\n",
            "5. Q: What are the methods recommended for using SVM with sparse data?\n",
            "A: To predict with sparse data, fit an SVM on the complete sparse dataset and utilize type-float64 data. Possible formats include numpy.ndarray (dense) or scipy.sparse.csr_matrix (sparse).\n",
            "\n",
            "Let me know if you'd like more questions and answers!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "chain.get_graph().print_ascii()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "24l2vzhzj94f",
        "outputId": "5d15acc2-9ad6-4cc1-84db-7d79851cfee5"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "            +---------------------------+            \n",
            "            | Parallel<notes,quiz>Input |            \n",
            "            +---------------------------+            \n",
            "                 **               **                 \n",
            "              ***                   ***              \n",
            "            **                         **            \n",
            "+----------------+                +----------------+ \n",
            "| PromptTemplate |                | PromptTemplate | \n",
            "+----------------+                +----------------+ \n",
            "          *                               *          \n",
            "          *                               *          \n",
            "          *                               *          \n",
            "  +------------+                 +-----------------+ \n",
            "  | ChatOpenAI |                 | ChatHuggingFace | \n",
            "  +------------+                 +-----------------+ \n",
            "          *                               *          \n",
            "          *                               *          \n",
            "          *                               *          \n",
            "+-----------------+              +-----------------+ \n",
            "| StrOutputParser |              | StrOutputParser | \n",
            "+-----------------+              +-----------------+ \n",
            "                 **               **                 \n",
            "                   ***         ***                   \n",
            "                      **     **                      \n",
            "           +----------------------------+            \n",
            "           | Parallel<notes,quiz>Output |            \n",
            "           +----------------------------+            \n",
            "                          *                          \n",
            "                          *                          \n",
            "                          *                          \n",
            "                 +----------------+                  \n",
            "                 | PromptTemplate |                  \n",
            "                 +----------------+                  \n",
            "                          *                          \n",
            "                          *                          \n",
            "                          *                          \n",
            "                   +------------+                    \n",
            "                   | ChatOpenAI |                    \n",
            "                   +------------+                    \n",
            "                          *                          \n",
            "                          *                          \n",
            "                          *                          \n",
            "                +-----------------+                  \n",
            "                | StrOutputParser |                  \n",
            "                +-----------------+                  \n",
            "                          *                          \n",
            "                          *                          \n",
            "                          *                          \n",
            "              +-----------------------+              \n",
            "              | StrOutputParserOutput |              \n",
            "              +-----------------------+              \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "CQ9NSuTqklPw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Conditional Chain"
      ],
      "metadata": {
        "id": "pGX447ArlOVk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "from langchain_core.prompts import PromptTemplate\n",
        "from langchain_core.output_parsers import StrOutputParser\n",
        "from langchain.schema.runnable import RunnableParallel, RunnableBranch, RunnableLambda\n",
        "from langchain_core.output_parsers import PydanticOutputParser\n",
        "from pydantic import BaseModel, Field\n",
        "from typing import Literal\n",
        "\n",
        "\n",
        "model = ChatOpenAI(api_key = api_key)\n",
        "\n",
        "parser = StrOutputParser()\n",
        "\n",
        "class Feedback(BaseModel):\n",
        "\n",
        "    sentiment: Literal['positive', 'negative'] = Field(description='Give the sentiment of the feedback')\n",
        "\n",
        "parser2 = PydanticOutputParser(pydantic_object=Feedback)\n",
        "\n",
        "prompt1 = PromptTemplate(\n",
        "    template='Classify the sentiment of the following feedback text into postive or negative \\n {feedback} \\n {format_instruction}',\n",
        "    input_variables=['feedback'],\n",
        "    partial_variables={'format_instruction':parser2.get_format_instructions()}\n",
        ")\n",
        "\n",
        "classifier_chain = prompt1 | model | parser2\n",
        "\n",
        "prompt2 = PromptTemplate(\n",
        "    template='Write an appropriate response to this positive feedback \\n {feedback}',\n",
        "    input_variables=['feedback']\n",
        ")\n",
        "\n",
        "prompt3 = PromptTemplate(\n",
        "    template='Write an appropriate response to this negative feedback \\n {feedback}',\n",
        "    input_variables=['feedback']\n",
        ")\n",
        "\n",
        "branch_chain = RunnableBranch(\n",
        "    (lambda x:x.sentiment == 'positive', prompt2 | model | parser),\n",
        "    (lambda x:x.sentiment == 'negative', prompt3 | model | parser),\n",
        "    RunnableLambda(lambda x: \"could not find sentiment\")\n",
        ")\n",
        "\n",
        "chain = classifier_chain | branch_chain\n"
      ],
      "metadata": {
        "id": "ZN921js5lQLT"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(chain.invoke({'feedback': 'This is a very bad phone'}))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QzAwTdTAlYY0",
        "outputId": "577136f2-a867-4273-a539-83ccc3db9dae"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "I'm sorry to hear that you had a negative experience. Please let us know how we can improve and make things right for you. Your feedback is valuable to us.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "chain.get_graph().print_ascii()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7n8z_gBTlcke",
        "outputId": "39c05c68-f04f-445c-8112-5df2c533d268"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "    +-------------+      \n",
            "    | PromptInput |      \n",
            "    +-------------+      \n",
            "            *            \n",
            "            *            \n",
            "            *            \n",
            "   +----------------+    \n",
            "   | PromptTemplate |    \n",
            "   +----------------+    \n",
            "            *            \n",
            "            *            \n",
            "            *            \n",
            "     +------------+      \n",
            "     | ChatOpenAI |      \n",
            "     +------------+      \n",
            "            *            \n",
            "            *            \n",
            "            *            \n",
            "+----------------------+ \n",
            "| PydanticOutputParser | \n",
            "+----------------------+ \n",
            "            *            \n",
            "            *            \n",
            "            *            \n",
            "       +--------+        \n",
            "       | Branch |        \n",
            "       +--------+        \n",
            "            *            \n",
            "            *            \n",
            "            *            \n",
            "    +--------------+     \n",
            "    | BranchOutput |     \n",
            "    +--------------+     \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(chain.invoke({'feedback': 'This is a fantastic phone'}))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qpSoLoCll25W",
        "outputId": "2c19257a-52e2-4a7b-a4fb-7350d6b11df2"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Thank you so much for your kind words! We are thrilled to hear that you had a positive experience with us. We strive to provide excellent service and we are glad that we were able to meet your expectations. If you have any other feedback or suggestions for us, please feel free to share. Thank you for your support!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "chain.get_graph().print_ascii()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "djy_guKyl_uf",
        "outputId": "b6d2cc8b-5da0-46ca-f5dd-c91fec1f5a9c"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "    +-------------+      \n",
            "    | PromptInput |      \n",
            "    +-------------+      \n",
            "            *            \n",
            "            *            \n",
            "            *            \n",
            "   +----------------+    \n",
            "   | PromptTemplate |    \n",
            "   +----------------+    \n",
            "            *            \n",
            "            *            \n",
            "            *            \n",
            "     +------------+      \n",
            "     | ChatOpenAI |      \n",
            "     +------------+      \n",
            "            *            \n",
            "            *            \n",
            "            *            \n",
            "+----------------------+ \n",
            "| PydanticOutputParser | \n",
            "+----------------------+ \n",
            "            *            \n",
            "            *            \n",
            "            *            \n",
            "       +--------+        \n",
            "       | Branch |        \n",
            "       +--------+        \n",
            "            *            \n",
            "            *            \n",
            "            *            \n",
            "    +--------------+     \n",
            "    | BranchOutput |     \n",
            "    +--------------+     \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "aqKO7oO0mELy"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}